{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\A_Category\\\\iNeuron\\\\End-To-End-NLP-Project-News-Article-Sorting\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\A_Category\\\\iNeuron\\\\End-To-End-NLP-Project-News-Article-Sorting'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen= True)\n",
    "class ModeTrainerConfig:\n",
    "  root_dir: Path\n",
    "  train_data_path: Path\n",
    "  val_data_path: Path\n",
    "  model_ckpt: Path\n",
    "  output_dir: Path\n",
    "  learning_rate: float\n",
    "  per_device_train_batch_size: int\n",
    "  per_device_eval_batch_size: int\n",
    "  num_train_epochs: int\n",
    "  weight_decay: float\n",
    "  eval_steps: int\n",
    "  evaluation_strategy: str\n",
    "  save_strategy: str\n",
    "  load_best_model_at_end: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ArticleSorting.constants import *\n",
    "from ArticleSorting.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, \n",
    "                 config_filepath = CONFIG_FILE_PATH,\n",
    "                 params_filepath = PARAMS_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_trainer_config(self) -> ModeTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.TrainingArguments\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModeTrainerConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            train_data_path = config.train_data_path,\n",
    "            val_data_path = config.val_data_path,\n",
    "            model_ckpt = config.model_ckpt,\n",
    "            output_dir = params.output_dir,\n",
    "            learning_rate  = params.learning_rate,\n",
    "            per_device_train_batch_size  = params.per_device_train_batch_size,\n",
    "            per_device_eval_batch_size  = params.per_device_eval_batch_size,\n",
    "            num_train_epochs  = params.num_train_epochs,\n",
    "            weight_decay= params.weight_decay,\n",
    "            eval_steps= params.eval_steps,\n",
    "            evaluation_strategy= params.evaluation_strategy,\n",
    "            save_strategy = params.save_strategy,\n",
    "            load_best_model_at_end= params.load_best_model_at_end\n",
    "        )\n",
    "\n",
    "        return model_trainer_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModeTrainer:\n",
    "    def __init__(self, config: ModeTrainerConfig) :\n",
    "        self.config = config\n",
    "\n",
    "    def train(self):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(device)\n",
    "        # Empty cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Loading data\n",
    "        train_dataset = load_from_disk(self.config.train_data_path)\n",
    "        val_dataset = load_from_disk(self.config.val_data_path)\n",
    "    \n",
    "        # DataLoader\n",
    "        #train_dataloader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=4)\n",
    "        #eval_dataloader = DataLoader(dataset=test_dataset, batch_size=4)\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.model_ckpt)\n",
    "        id2label = {0: \"business\", 1: \"entertainment\", 2: \"politics\", 3: \"sport\", 4: \"tech\"}\n",
    "        label2id = {\"business\": 0, \"entertainment\": 1, \"politics\": 2, \"sport\": 3, \"tech\": 4 }\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.config.model_ckpt,\n",
    "            num_labels=5,\n",
    "            id2label=id2label, \n",
    "            label2id=label2id\n",
    "            ).to(device)\n",
    "        \n",
    "        \n",
    "        accuracy = evaluate.load(\"accuracy\")\n",
    "        def compute_metrics(eval_pred):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.argmax(predictions, axis=1)\n",
    "            return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "        \n",
    "                \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"bert-base-cased\",\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=4,\n",
    "            per_device_eval_batch_size=4,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=0.01,\n",
    "            eval_steps = 10,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=test_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        trainer.evaluate()\n",
    "\n",
    "         ## Save model\n",
    "        model.save_pretrained(os.path.join(self.config.root_dir,\"bert-base-uncased-model\"))\n",
    "        ## Save tokenizer\n",
    "        tokenizer.save_pretrained(os.path.join(self.config.root_dir,\"tokenizer\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-28 23:35:51,839:  INFO: common: yaml file:config\\config.yaml loaded successfully]\n",
      "[2023-10-28 23:35:51,885:  INFO: common: yaml file:params.yaml loaded successfully]\n",
      "[2023-10-28 23:35:51,898:  INFO: common: created directory at : artifacts]\n",
      "[2023-10-28 23:35:51,936:  INFO: common: created directory at : artifacts/model_trainer]\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/745 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "                                                   \n",
      " 20%|██        | 149/745 [52:30<3:00:28, 18.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.10122085362672806, 'eval_accuracy': 0.9798657718120806, 'eval_runtime': 255.5003, 'eval_samples_per_second': 1.166, 'eval_steps_per_second': 0.149, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 40%|████      | 298/745 [1:43:49<2:07:35, 17.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07058887183666229, 'eval_accuracy': 0.9865771812080537, 'eval_runtime': 230.4811, 'eval_samples_per_second': 1.293, 'eval_steps_per_second': 0.165, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 411/745 [2:16:16<1:37:56, 17.59s/it] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\A_Category\\iNeuron\\End-To-End-NLP-Project-News-Article-Sorting\\research\\04_model_trainer.ipynb Cell 10\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/04_model_trainer.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     model_trainer_comfig \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget_model_trainer_config()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/04_model_trainer.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     model_trainer \u001b[39m=\u001b[39m ModeTrainer(config\u001b[39m=\u001b[39m model_trainer_comfig)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/04_model_trainer.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     model_trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/04_model_trainer.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/04_model_trainer.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "\u001b[1;32md:\\A_Category\\iNeuron\\End-To-End-NLP-Project-News-Article-Sorting\\research\\04_model_trainer.ipynb Cell 10\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/04_model_trainer.ipynb#X12sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/04_model_trainer.ipynb#X12sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbert-base-cased\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/04_model_trainer.ipynb#X12sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39m\u001b[39m2e-5\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/04_model_trainer.ipynb#X12sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/04_model_trainer.ipynb#X12sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/04_model_trainer.ipynb#X12sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/04_model_trainer.ipynb#X12sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/04_model_trainer.ipynb#X12sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/04_model_trainer.ipynb#X12sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/04_model_trainer.ipynb#X12sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/04_model_trainer.ipynb#X12sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/04_model_trainer.ipynb#X12sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m  \u001b[39m## Save model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/04_model_trainer.ipynb#X12sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m model\u001b[39m.\u001b[39msave_pretrained(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mroot_dir,\u001b[39m\"\u001b[39m\u001b[39mbert-base-uncased-model\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Tanjina\\anaconda3\\envs\\textSort\\lib\\site-packages\\transformers\\trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1534\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1536\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1537\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1538\u001b[0m )\n\u001b[1;32m-> 1539\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1540\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1541\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1542\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1543\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1544\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Tanjina\\anaconda3\\envs\\textSort\\lib\\site-packages\\transformers\\trainer.py:1811\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1808\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m   1809\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m-> 1811\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1812\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1813\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1814\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1815\u001b[0m ):\n\u001b[0;32m   1816\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1817\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   1818\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_comfig = config.get_model_trainer_config()\n",
    "    model_trainer = ModeTrainer(config= model_trainer_comfig)\n",
    "    model_trainer.train()\n",
    "\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
