{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\A_Category\\\\iNeuron\\\\End-To-End-NLP-Project-News-Article-Sorting\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\A_Category\\\\iNeuron\\\\End-To-End-NLP-Project-News-Article-Sorting'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen = True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    tokenizer_name: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ArticleSorting.constants import *\n",
    "from ArticleSorting.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, \n",
    "                 config_filepath = CONFIG_FILE_PATH,\n",
    "                 params_filepath = PARAMS_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self) ->DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path= config.data_path,\n",
    "            tokenizer_name=config.tokenizer_name\n",
    "        )\n",
    "\n",
    "        return data_transformation_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tanjina\\anaconda3\\envs\\textSort\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ArticleSorting.logging import logger\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import Dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)\n",
    "        \n",
    "\n",
    "    def encode_categories(self):\n",
    "        df = pd.read_csv(self.config.data_path)\n",
    "        df['encoded_label'] = df['Category'].astype('category').cat.codes\n",
    "\n",
    "        ## Spliting the Data\n",
    "        # Training dataset\n",
    "        train_data = df.sample(frac=0.8, random_state=42)\n",
    "        # Testing dataset\n",
    "        test_data = df.drop(train_data.index)\n",
    "\n",
    "        # Convert pyhton dataframe to Hugging Face arrow dataset\n",
    "        hg_train_data = Dataset.from_pandas(train_data)\n",
    "        hg_test_data = Dataset.from_pandas(test_data)\n",
    "        print(hg_train_data, hg_test_data)\n",
    "        return hg_train_data, hg_test_data\n",
    "   \n",
    "    def tokenize_dataset(self, data):\n",
    "        return self.tokenizer(data[\"Text\"],\n",
    "                     max_length=512,\n",
    "                     truncation=True,\n",
    "                     padding=\"max_length\")\n",
    "    \n",
    "    def convert(self):\n",
    "        hg_train_data, hg_test_data = self.encode_categories()\n",
    "\n",
    "        # Tokenize the dataset\n",
    "        dataset_train = hg_train_data.map(self.tokenize_dataset)\n",
    "        dataset_test = hg_test_data.map(self.tokenize_dataset)\n",
    "        \n",
    "        # Remove the review and index columns because it will not be used in the model\n",
    "        dataset_train = dataset_train.remove_columns([\"ArticleId\", \"Text\", \"Category\", \"__index_level_0__\"])\n",
    "        dataset_test = dataset_test.remove_columns([\"ArticleId\", \"Text\", \"Category\", \"__index_level_0__\"])\n",
    "\n",
    "        # Rename label to labels because the model expects the name labels\n",
    "        dataset_train = dataset_train.rename_column(\"encoded_label\", \"labels\")\n",
    "        dataset_test = dataset_test.rename_column(\"encoded_label\", \"labels\")\n",
    "\n",
    "        # Change the format to PyTorch tensors\n",
    "        dataset_train.set_format(\"torch\")\n",
    "        dataset_test.set_format(\"torch\")\n",
    "\n",
    "        # Take a look at the data\n",
    "        print(dataset_train)\n",
    "        print(dataset_test)\n",
    "\n",
    "        dataset_train.save_to_disk(os.path.join(self.config.root_dir,\"Train BBC dataset\"))\n",
    "        dataset_test.save_to_disk(os.path.join(self.config.root_dir,\"Test BBC dataset\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-28 20:54:16,764:  INFO: common: yaml file:config\\config.yaml loaded successfully]\n",
      "[2023-10-28 20:54:16,769:  INFO: common: yaml file:params.yaml loaded successfully]\n",
      "[2023-10-28 20:54:16,773:  INFO: common: created directory at : artifacts]\n",
      "[2023-10-28 20:54:16,775:  INFO: common: created directory at : artifacts/data_transformation]\n",
      "Dataset({\n",
      "    features: ['ArticleId', 'Text', 'Category', 'encoded_label', '__index_level_0__'],\n",
      "    num_rows: 1192\n",
      "}) Dataset({\n",
      "    features: ['ArticleId', 'Text', 'Category', 'encoded_label', '__index_level_0__'],\n",
      "    num_rows: 298\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 1192\n",
      "})\n",
      "Dataset({\n",
      "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 298\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.convert()\n",
    "  \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
