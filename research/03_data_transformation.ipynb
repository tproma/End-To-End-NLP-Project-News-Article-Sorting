{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\A_Category\\\\iNeuron\\\\End-To-End-NLP-Project-News-Article-Sorting\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\A_Category\\\\iNeuron\\\\End-To-End-NLP-Project-News-Article-Sorting'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen = True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    tokenizer_name: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ArticleSorting.constants import *\n",
    "from ArticleSorting.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, \n",
    "                 config_filepath = CONFIG_FILE_PATH,\n",
    "                 params_filepath = PARAMS_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self) ->DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path= config.data_path,\n",
    "            tokenizer_name=config.tokenizer_name\n",
    "        )\n",
    "\n",
    "        return data_transformation_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ArticleSorting.logging import logger\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import Dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)\n",
    "        self.df = pd.read_csv(self.config.data_path)\n",
    "\n",
    "\n",
    "    def encode_categories(self):\n",
    "        self.df[\"encoded_label\"] = self.df[\"Category\"].astype(\"category\").cat.codes\n",
    "        hg_data = Dataset.from_pandas(self.df)\n",
    "        return hg_data\n",
    "   \n",
    "    def tokenize_dataset(self, data):\n",
    "        return self.tokenizer(data[\"Text\"],\n",
    "                     max_length=512,\n",
    "                     truncation=True,\n",
    "                     padding=\"max_length\")\n",
    "    \n",
    "    def convert(self):\n",
    "        data = self.encode_categories()\n",
    "        tokenized_data = data.map(self.tokenize_dataset)\n",
    "        # Remove  columnms\n",
    "        tokenized_data = tokenized_data.remove_columns([\"ArticleId\", \"Text\", \"Category\", \"__index_level_0__\"])\n",
    "        # Rename label to labels because the model expects the name labels\n",
    "        dataset = tokenized_data.rename_column(\"encoded_label\", \"labels\")\n",
    "        # Change the format to PyTorch tensors\n",
    "        dataset.set_format(\"torch\")\n",
    "        print(dataset)\n",
    "\n",
    "        dataset.save_to_disk(os.path.join(self.config.root_dir,\"BBC dataset\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-26 23:12:40,439:  INFO: common: yaml file:config\\config.yaml loaded successfully]\n",
      "[2023-10-26 23:12:40,443:  INFO: common: yaml file:params.yaml loaded successfully]\n",
      "[2023-10-26 23:12:40,447:  INFO: common: created directory at : artifacts]\n",
      "[2023-10-26 23:12:40,450:  INFO: common: created directory at : artifacts/data_transformation]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Column name __index_level_0__ not in the dataset. Current columns in the dataset: ['ArticleId', 'Text', 'Category', 'encoded_label', 'input_ids', 'token_type_ids', 'attention_mask']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\A_Category\\iNeuron\\End-To-End-NLP-Project-News-Article-Sorting\\research\\03_data_transformation.ipynb Cell 10\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     data_transformation\u001b[39m.\u001b[39mconvert()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "\u001b[1;32md:\\A_Category\\iNeuron\\End-To-End-NLP-Project-News-Article-Sorting\\research\\03_data_transformation.ipynb Cell 10\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     data_transformation_config \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget_data_transformation_config()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     data_transformation \u001b[39m=\u001b[39m DataTransformation(config\u001b[39m=\u001b[39mdata_transformation_config)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     data_transformation\u001b[39m.\u001b[39;49mconvert()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "\u001b[1;32md:\\A_Category\\iNeuron\\End-To-End-NLP-Project-News-Article-Sorting\\research\\03_data_transformation.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m tokenized_data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mmap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenize_dataset)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Remove  columnms\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m tokenized_data \u001b[39m=\u001b[39m tokenized_data\u001b[39m.\u001b[39;49mremove_columns([\u001b[39m\"\u001b[39;49m\u001b[39mArticleId\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mText\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mCategory\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m__index_level_0__\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Rename label to labels because the model expects the name labels\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/A_Category/iNeuron/End-To-End-NLP-Project-News-Article-Sorting/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m dataset \u001b[39m=\u001b[39m tokenized_data\u001b[39m.\u001b[39mrename_column(\u001b[39m\"\u001b[39m\u001b[39mencoded_label\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Tanjina\\anaconda3\\envs\\textSort\\lib\\site-packages\\datasets\\arrow_dataset.py:580\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    578\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    579\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 580\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    581\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    582\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[0;32m    583\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tanjina\\anaconda3\\envs\\textSort\\lib\\site-packages\\datasets\\arrow_dataset.py:545\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    538\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[0;32m    539\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[0;32m    540\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[0;32m    541\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[0;32m    542\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[0;32m    543\u001b[0m }\n\u001b[0;32m    544\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 545\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    546\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    547\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tanjina\\anaconda3\\envs\\textSort\\lib\\site-packages\\datasets\\fingerprint.py:511\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    507\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[0;32m    509\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[1;32m--> 511\u001b[0m out \u001b[39m=\u001b[39m func(dataset, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    513\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[0;32m    515\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tanjina\\anaconda3\\envs\\textSort\\lib\\site-packages\\datasets\\arrow_dataset.py:2145\u001b[0m, in \u001b[0;36mDataset.remove_columns\u001b[1;34m(self, column_names, new_fingerprint)\u001b[0m\n\u001b[0;32m   2143\u001b[0m \u001b[39mfor\u001b[39;00m column_name \u001b[39min\u001b[39;00m column_names:\n\u001b[0;32m   2144\u001b[0m     \u001b[39mif\u001b[39;00m column_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m dataset\u001b[39m.\u001b[39m_data\u001b[39m.\u001b[39mcolumn_names:\n\u001b[1;32m-> 2145\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2146\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mColumn name \u001b[39m\u001b[39m{\u001b[39;00mcolumn_name\u001b[39m}\u001b[39;00m\u001b[39m not in the dataset. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2147\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCurrent columns in the dataset: \u001b[39m\u001b[39m{\u001b[39;00mdataset\u001b[39m.\u001b[39m_data\u001b[39m.\u001b[39mcolumn_names\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2148\u001b[0m         )\n\u001b[0;32m   2150\u001b[0m \u001b[39mfor\u001b[39;00m column_name \u001b[39min\u001b[39;00m column_names:\n\u001b[0;32m   2151\u001b[0m     \u001b[39mdel\u001b[39;00m dataset\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mfeatures[column_name]\n",
      "\u001b[1;31mValueError\u001b[0m: Column name __index_level_0__ not in the dataset. Current columns in the dataset: ['ArticleId', 'Text', 'Category', 'encoded_label', 'input_ids', 'token_type_ids', 'attention_mask']"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.convert()\n",
    "  \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
